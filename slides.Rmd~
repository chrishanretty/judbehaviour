% Judicial behaviour part, 1
% Chris Hanretty
% 6th May 2016

```{r setopts, echo = FALSE}
library(knitr)
library(memisc)
library(pander)
opts_chunk$set(echo = FALSE, results = "hide",
               message = FALSE, warning = FALSE)

```

# Introduction

## About this course

 - to make inferences about judges from their behaviour
 - in two parts
   * regression-based methods
   * ideal point analysis
 - Regression based methods will start off with simple scenarios
 
## About me

 - Reader in Politics, University of East Anglia
 - I've published on judicial politics in the UK and elsewhere
 - *I am not a lawyer*
 
## About you

 - I assume no knowledge of statistics
 - You may want to *use* these models
 - I have provided `R` code for you
 - You may want to *understand* these models
 - The code will still help
 
## Outline

 - Random assignment of a single judge to cases, with a continuous outcome
 - Random assignment of a single judge to cases, with a dichotomous outcome
 - Random assignment of multiple judges to cases, dichotomous outcome
 - Random assignment of multiple judges: some complications
 
# Continuous outcomes

## Continuous outcomes in law

 - Continuous outcomes are common
	* Fines (amount of fine in currency)
	* Sentences (length of sentence in months)
	* Damages (damages award)
	* Negligence assessment (% liability)
 - Continuous outcomes are easy to model
 - We use the tools of linear regression
 
## Galton's children

```{r galtonfig, fig = TRUE, fig.cap = "Galton's height data"}
galton <- read.csv("data/Galton.csv")
plot(galton$Father, galton$Height,
     xlab = "Child's height [inches]",
     ylab = "Father's height [inches]",
     pch = 19,
          col = 'darkgrey')

```

From: Galton, F. (1886). Regression towards mediocrity in hereditary stature. *The Journal of the Anthropological Institute of Great Britain and Ireland*, 15, 246-263.

## A linear regression equation

 - We can summarize the trend by using the equation for a straight line
 
$$
y = a + bx
$$

 - I'll refer to 
	 * $y$ as the dependent variable (in this case children's height)
	 * $x$ as the independent variable (parents' height)
	 * $a$ as an intercept
	 * $b$ as a coefficient
	 
## The best-fitting line

```{r galton-bestfit, fig = TRUE, fig.cap = "Galton's height data"}

mod <- lm(Height ~ Father, data = galton)

plot(galton$Father, galton$Height,
     xlab = "Child's height [inches]",
     ylab = "Father's height [inches]",
     pch = 19,
          col = 'darkgrey')

abline(mod)

text(65, 77,
     label = paste0("y = ",
         round(coef(mod)[1],2),
         " + ",
         round(coef(mod)[2],2),
         " * x"),
     pos = 1)
```

## Interpretation

 - When the value of the independent variable is zero, height is equal to $a$ inches
 - For every unit increase in the independent, height increases by $b$ inches
 - ... other things equal

## Interpretation

 - When the value of the independent variable is zero, height is equal to 39.11 inches
 - For every unit increase in the independent, height increases by 0.4 inches
 - ... other things equal

## For judges

 - Height can be measured by numbers
 - How can judges be measured by numbers?
 - Solution: create dummy variables
 
## An example

 - Suppose cases are heard by two judges: Tender and Terrible
 - We set one of these judges as the *reference category*
 - Suppose *Tender* is the reference category
 - We create a dummy variable which has value zero if Tender heard the case
 - ... and one if *Terrible* heard the case

## Graphing Tender and Terrible

```{r tendergraph, fig.cap = "Sentence lengths"}
tender <- read.csv("data/fakedata.csv")

plot(jitter(tender$JudgeTerrible), tender$Sentence,
     xlab = "Judge is Judge Terrible",
     ylab = "Sentence length (months)",
     pch = 19,
     col = "darkgray",
     xlim = c(-2,2),
     axes = FALSE)
axis(1, at = c(0,1))
axis(2)


```

## The best-fitting line


```{r tenderbestfit, fig.cap = "Sentence lengths with best-fitting line"}
plot(jitter(tender$JudgeTerrible), tender$Sentence,
     xlab = "Judge is Judge Terrible",
     ylab = "Sentence length (months)",
     pch = 19,
     col = "darkgray",
     xlim = c(-2,2),
     axes = FALSE)
axis(1, at = c(0,1))
axis(2)

mod <- lm(Sentence ~ JudgeTerrible, data = tender)
abline(mod)
     
```



## Interpretation

 - When the value of the independent variable is zero (i.e., Tender heard the case), average sentence length is just under ten months
 - For each unit increase in the variable `terrible`, sentence length increases by 2.2 months
 - Alternatively: Judge Terrible imposes sentences that are 2.2 months longer than Tender.
 - This difference cannot be due to the cases heard by Terrible
 - if Terrible systematically heard cases which required longer sentences, no randomization!

## How does it look? The data

```{r fakedatashow, echo = TRUE, results= "markup"}
head(tender)
```

## How does it look? The command

```{r tendermod, echo = TRUE}
tender <- read.csv("data/fakedata.csv")
mod <- lm(Sentence ~ JudgeTerrible, data = tender)
```

 - `mod`: an arbitrary name
 - `lm`: short for *l*inear *m*odel
 - `y~x`: dependent variable, then a tilde, then independent variables
 - `data=tender`: tell R where to find our data

## How does it look? The output

\small
```{r tendermodout, results = "markup", echo = TRUE}
summary(mod)
```

## How does it look when published?

\small
```{r tendermodpretty, results = "markup", echo = FALSE}
pander(mtable(mod,
              summary.stats = c("N","AIC","R-squared")))
```

## Extensions

 - More than two judges: create J-1 dummy variables, where $J$ is the number of unique judges
 - Additional control variables: *not necessary*, but helpful. 
 - Such variables should not change the judge coefficients dramatically
 - Transform our dependent variable by taking the natural log
 - Why? Sometimes our dependent variable is strictly positive.
 
## A model with a logged dependent variable


```{r tendermodlogged, echo = FALSE, results="asis"}
mod <- lm(log(Sentence) ~ JudgeTerrible, data = tender)
pander(mtable(mod,
              summary.stats = c("N","AIC","R-squared")))
```

Judge Terrible increases sentences by a *factor* of $e^`r round(coef(mod)[2],3)` = `r round(exp(coef(mod)[2]),3)`$. 
Given that average sentence $\sim$ 10 months, this makes sense.
Approximately: coefficient $\times 100$ = the percentage change.




# Dichotomous outcomes

## Examples in law

 - Dichotomous outcomes are also common:
	 * The appeal is either *allowed* or *dismissed*
	 * The plaintiff either *wins* or *loses*
	 * The defendant is either *guilty* or *innocent*
	 * The sentence is either *prison* or a *non-custodial sentence*
	 * (The case is decided in a *liberal* or *conservative* direction)
 - These require a different modelling strategy
 
## A new formula

We need:

 - a formula that models the *probability* of one of two possible outcomes
 - a formula which keeps within logically possible bounds (0-1)
 
Before we had:

$$
y = a + bx
$$

Now we have

$$
Prob(y = 1) = \frac{1}{1 + e^{-(a+bx)}}
$$
 
where $e$ is the exponential operator, or anti-log. 

## Visually...

$a$ = something which makes the outcome more likely when it is higher

![Different values of $a$](figure/logitplots1-1.png)

## Visually...

$b$ = the degree to which the outcome depends on $x$

![Different values of $b$](figure/logitplots2-1.png)

## An application

> Gaudet, Frederick, Harris, George S., and St. John, Charles W. (1933), "Individual differences in the sentencing tendencies of judges", *American Institute of Criminal Law and Criminology*, 23(5), 811-818. 

To my knowledge, the earliest use of random assignment of judges to investigate judge effects.

## The logic, explained

> "Since the rule is that there is no selection of the cases which the
judge is to sentence, but that the sentencing of a particular prisoner
by a particular judge is a matter of chance (the judges rotate), it is
obvious that, by chance, each judge should get an equal number of
cases whose sentences would normally be long or short...  Given a
sufficiently large number of cases, if one finds that the average
severity of the sentences of two judges is appreciably different, one
is justified in saying that the factors which determine this
difference in the sentencing tendencies are to be found outside of the
circumstances of the crime and those of the prisoner, and hence
probably in the judge since he is the other factor which is always
present".

## The table

![Gaudet et al., Fig. 1](figure/gaudet_tab1.png)

## Running a model

 - My focus is on the decision to imprison
 - I'll assume this is data from a sample, not the entire population
 - Interest is not in the differences, but in their statistical significance

## How it looks: the data

```{r gaudetin, echo = TRUE, results = "markup"}
gaudet <- read.csv("data/gaudet.csv")
head(gaudet)
```

## How it looks, the model

```{r gaudetmod, echo = TRUE, results = "markup"}
mod <- glm(I(Sentence == "Imprisonment")~Judge,
           family = binomial,
           data = gaudet)
```

where 

 - `glm`: short for *g*eneralized *l*inear *m*odel
 - `I(.)`: what follows isn't just a variable name
 - `family = binomial`: this is a logistic regression, not some other model
 
## How it looks, the results

\footnotesize
```{r gaudetout, results = "markup", echo = T}
summary(mod)
```
 
## How it looks, as published



\footnotesize
```{r gaudetpretty, results = "asis", echo = FALSE}
tmp <- mtable(mod)
tmp$summaries <- subset(tmp$summaries,
                        rownames(tmp$summaries) %in% c("N","AIC","Log-likelihood","Nagelkerke R-sq."))
pander(tmp)
```

## Interpretation

 - The coefficient on Judge 3 is `r round(coef(mod)[3],3)`
 - This means that Judge 3 is $e^{`r round(coef(mod)[3],3)`} = `r round(exp(coef(mod)[3]),3)`$ times more likely to send people to prison.
 - That makes sense given the figures listed in Gaudet et al's Table 1.
 
# Multiple judges

## Our new source of data

![Sunstein et al, *Are judges political?*](figure/sunstein_cover.jpg)

## The dependent variable

 - The vote of an individual judge in a case heard by multiple judges
  - These votes can be coded in different ways
  - Here, the focus is on "stereotypically liberal" decisions
  
## The independent variable

 - A dummy variable, which has value one if the judge was appointed by a Democratic president
 - In the book, this is elided with ideology
 - "Party of the appointing actor" may be a poor guide in other contexts
 
## The key assumption

 - For the book to work, it's necessary for judges to be assigned to cases on a random basis
 - *Within each circuit*, judges are assigned randomly
 - So *within a circuit*, we can just use judge dummies
 
## A simple example

 - Let's focus on cases:
	 * involving the Americans with Disabilities Act
	 * in the 7th Circuit
 - Why? Most common circuit-casetype combination

```{r sunsteinin, echo = TRUE}
dat <- read.csv("data/Ch4Sunstein.csv")
ada <- subset(dat, circuit == 7 & dataset == 2)
head(ada[,c("judge_name", "conserve_vote", "cite", "dec_year", "party_pres")])
```

## A simple model

\footnotesize

```{r adamod, echo = TRUE, results = "markup"}
mod <- glm(conserve_vote~party_pres,
           data = ada,
           family = binomial)
summary(mod)
```

## Multiple circuits

 - Here, we have results that are not significant at commonly-accepted levels
 - We have additional data we can use -- data from other circuits
 - However, judges are not randomly-assigned to cases *across circuits*
 - In particular, the chances of a Democrat sitting on certain cases is higher on certain circuits
 
## Testing random assignment *across* circuits

 - Let's model `party_pres` as a function of circuit!
 

```{r reversemod, echo = TRUE}
mod <- glm(party_pres ~ factor(circuit),
           data = dat,
           family = binomial)
```

## The "reverse" model

\scriptsize
```{r reversemodout, results = "markup"}
summary(mod)
```

## How to proceed?

 - We have seen the chances of a Democrat sitting on a case varies according to circuit
 - Differences in the rates at which Democrats vote certain ways may therefore be due not to party...
 - but to the types of circuits which Democrats sit on, and the types of cases heard by those circuits
 - ... but if we were prepared to say that the chance of a Democrat was random *conditional* on knowing the circuit?
 
## A fuller model

```{r modwithcircuits, echo = TRUE, results = "markup"}
mod <- glm(conserve_vote~party_pres+factor(circuit),
           data = dat,
           family = binomial)
``` 

## The fuller model, results

\tiny
```{r modwithcircuitsout, echo = TRUE, results = "markup"}
summary(mod)
```

## Problems with the analysis in Sunstein et al.

 - Random allocation isn't true even within circuits!
 - For information on this, see:
 
> Hall, M. (2010). Randomness Reconsidered: Modeling Random Judicial Assignment in the US Courts of Appeals. *Journal of Empirical Legal Studies*, 7(3), 574-589.

 - Hall called the clerks in each circuit
 - He found that "[r]andom assignment was not used in the Fourth Circuit before the year 2000,the Fifth and Eighth Circuits before 2003, or the Tenth Circuit before 1998"
 - He also noted that partisan composition *of circuits* changes over time
 - When corrected for these oversights, the magnitude of the effect changes.
 - Always check the veracity of any random allocation assumption!
 

# Panel outcomes

## A further problem with Sunstein

 - Often, the idea of random assignment is bound up with the idea of a randomized controlled trial
 - In this context, experimental *subjects* are assigned to a treatment or control *condition*
 - Subsequent properties of the subjects are measured.
 - In Sunstein et al, cases are assign to a treatment (Democratic judge) or control (Republican judge)
 - ... but they receive other treatments (the other two judges on the panel)
 - and the outcome is measured at the level of the treatment (the judge), not the case!
 
## Explaining panel outcomes

 - Ideally, we'd like a way of explaining the effect of judges on panel outcomes
 - This is problematic, because it requires a theory of panel decision-making
 - Suppose the median voter decides, and we assign a third judge to an existing panel:
	 * If the two judges are RR, the median judge is R. Adding another R or D makes no difference
	 * If the two judges are DD, the median judge is D; the same reasoning follows
	 * If the two judges are RD or DR, adding another D or R will determine the median judge
 - Only in the last case does the judge matter
 - If we ran a regression, and if all circumstances were equally probably, our estimate would be biased downwards
 
## Options

 - Option 1 (low-tech): split the data up according to the partisanship of the first two judges:
	 * Pro: Easy to implement
	 * Pro: Recovers coefficients which link to the outcome
	 * Con: We're left with three effects
	 * Con: It's essential atheoretical
 - Option 2 (high-tech): posit a latent ideology associated with each judge, and take the median
	 * Pro: matches theory
	 * Pro: recovers a single estimate
	 * Con: requires some difficult-to-implement Bayesian statistics
	 
## Modelling panel outcomes

For more information on this type of approach, see:

Hangartner, Dominik, Benjamin E. Lauderdale, and Judith Spirig. "Refugee Roulette Revisited: Judicial Preference Variation and Aggregation on the Swiss Federal Administrative Court 2007-2012." (Working paper).

# Conclusions

## General

 - We've been able to exploit random assignment to learn something about judges
 - Given (limited, predictable) non-random assignment, we've been able to control for that, and still learn
 - We haven't discussed strongly non-random assignment (on the basis of experience, or other factors).

## Data

 - All the data is available on GitHub
 - The url is TODO:
 - The R code is also there
 - The Sunstein data is in any case available at [http://epstein.wustl.edu/research/behaviorJudges/chapters.php?req=11](http://epstein.wustl.edu/research/behaviorJudges/chapters.php?req=11)


